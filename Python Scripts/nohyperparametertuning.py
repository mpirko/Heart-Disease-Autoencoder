# -*- coding: utf-8 -*-
"""NoHyperparameterTuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BZbael-fq4TEb7u6Sf4_l7UcKB9VZGzu
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Datasets/
import pandas as pd
df = pd.read_csv('processed.cleveland.data')

#These are the packages the code requires for you to install
!pip install pandas
!pip install numpy
!pip install sklearn
!pip install matplotlib
!pip install tensorflow
!pip install keras
!pip install keras-tuner

#1: Age in Years
#2: Sex (0 = F, 1 = M) 
#3: Chest pain type (1 = typical angina, 2 = atypical angina, 3 = non-anginal pain, 4 = asymptomatic) 
#4: Resting blood pressure at admission (mmHg),
#5: Serum cholesterol (mmHg) 
#6: Fasting blood sugar > 120 mg/DL 1 = T, 0 = F
#7: Resting electrocardiographic results: 0 = normal, 1 = ST-T wave abnormality, 2 = probable/definite left ventricular hypertrophy
#8: Maximum heart rate achieved
#9: Exercise induced angina (1 = yes, 0 = no)
#10: ST depression induced by exercise relative to rest
#11: Slope of peak exercise ST segment (1 = upslope, 2 = flat, 3 = downslope)
#12: Number of major vessels (0-3) colored by fluoroscopy 
#13: Thalassemia (3 = normal, 6 = fixed defect, 7 = reversable defect)
#14: Diagnosis of Heart Disease (Angiographic Disease- 0: <50% diameter narrowing, 1: > 50% diameter narrowing)
df.columns = ['Age (Years)', 'Sex', 'Chest Pain Type', 'Resting Blood Pressure (mmHg)', 'Serum Cholesterol (mmHg)', 'Fasting Blood Sugar > 120 mg/dL', 'Resting Electrocardiographic Results', 'Maximum Heart Rate Achieved', 'Exercise-Induced Angina', 'ST Depression Induced by Exercise Relative to Rest', 'Slope of the Peak Exercise ST Segment','Number of Major Vessels Colored by Fluoroscopy', 'Thalassemia', 'Diagnosis of Heart Disease']

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import QuantileTransformer
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('processed.cleveland.data', header=None)
df.columns = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG',
              'MaxHR', 'ExerciseAngina', 'Oldpeak', 'STslope', 'MajorVessels', 'Thal', 'Target']

# Replace "?" with NaN
df.replace('?', np.nan, inplace=True)

# Convert data to float
df = df.astype(float)

# Drop rows with NaN values
print("With missing values: ", df.shape)
df.dropna(inplace=True)
print("Without missing values: ", df.shape)

# Select the features and target
X = df.iloc[:, :13].values
y = df.iloc[:, 13].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data
scaler = QuantileTransformer(n_quantiles=df.shape[0], output_distribution="uniform", random_state=1111)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

fig = plt.figure(figsize =(6, 3))
plt.boxplot(X_train)
plt.show()

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.model_selection import train_test_split
from kerastuner.tuners import RandomSearch
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('processed.cleveland.data', header=None)
df.columns = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG',
              'MaxHR', 'ExerciseAngina', 'Oldpeak', 'STslope', 'MajorVessels', 'Thal', 'Target']

# Replace "?" with NaN
df.replace('?', np.nan, inplace=True)

# Convert data to float
df = df.astype(float)

# Drop rows with NaN values
print("With missing values: ", df.shape)
df.dropna(inplace=True)
print("Without missing values: ", df.shape)

# Select the features and target
X = df.iloc[:, :13].values
y = df.iloc[:, 13].values

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize the data
scaler = QuantileTransformer(n_quantiles=df.shape[0], output_distribution="uniform", random_state=1111)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Define the input shape and the number of neurons in the latent space (d)
input_shape = X_train.shape[1:]
d = 5

# Define the encoder model
inputs = Input(shape=input_shape)
x = Dense(32, activation='relu')(inputs)
x = Dense(16, activation='relu')(x)
latent_space = Dense(d, activation='relu')(x)
encoder = Model(inputs, latent_space)

# Define the decoder model
decoder_inputs = Input(shape=(d,))
x = Dense(16, activation='relu')(decoder_inputs)
x = Dense(32, activation='relu')(x)
outputs = Dense(input_shape[0], activation='linear')(x)
decoder = Model(decoder_inputs, outputs)

# Define the autoencoder model by combining the encoder and decoder models
autoencoder_input = Input(shape=input_shape)
encoder_output = encoder(autoencoder_input)
decoder_output = decoder(encoder_output)
autoencoder = Model(autoencoder_input, decoder_output)

# Compile the autoencoder model using a suitable loss function (MSE)
autoencoder.compile(optimizer='adam', loss='mse')

# Fit the autoencoder model on the training set and validate on the test set
history = autoencoder.fit(X_train, X_train,
                          epochs=1000,
                          batch_size=32,
                          verbose=1,
                          validation_data=(X_test, X_test))
history_dict = history.history
for k,v in history_dict.items():
  plt.plot(v, label=k)
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend()

# Save the model as a .h5 file
model_filename = "NoHyperparameter.h5"
autoencoder.save(model_filename)

history_dict = history.history
for k,v in history_dict.items():
  plt.plot(v, label=k)
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend()

test_loss = autoencoder.evaluate(X_test, X_test)

history_dict = history.history
for k,v in history_dict.items():
  plt.plot(v, label=k)
plt.ylabel("Loss")
plt.xlabel("Epoch")
plt.legend()

# Predict the reconstructed output for each input
reconstructed_X = autoencoder.predict(X_test)

# Calculate the mean squared error between the original input and its reconstructed output for each feature separately
fmse = np.mean(np.square(reconstructed_X - X_test), axis=0)

# Create a list of feature names
feature_names = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS', 'RestingECG',
              'MaxHR', 'ExerciseAngina', 'Oldpeak', 'STslope', 'MajorVessels', 'Thal']

# Set the x-axis tick labels
plt.xticks(range(len(feature_names)), feature_names, rotation=90)

# Create a bar plot of the FMSE values
variance = np.var(X_train, axis=0).flatten()
plt.bar(feature_names, variance, label="variance")
plt.bar(feature_names, fmse, label="recon error", alpha=0.75)
plt.title('Feature-wise Mean Squared Error')
plt.xlabel('Feature')
plt.ylabel('FMSE')
plt.legend()
plt.show()

# Print the shape of X_test and the length of fmse
print("Shape of X_test:", X_test.shape)
print("Length of fmse:", len(fmse))

from sklearn.linear_model import LinearRegression

# Fit a linear regression model to the training data
reg = LinearRegression().fit(X_train, y_train)

# Get the coefficients (importances) of each feature
importances = reg.coef_

# Sort the features by importance (descending order)
sorted_idx = np.argsort(importances)[::-1]

# Print the feature importances
for i in sorted_idx:
    print(f"{feature_names[i]}: {importances[i]:.3f}")

# Sort the feature importances by importance (descending order)
sorted_idx = np.argsort(importances)[::-1]

# Create a bar plot of feature importances
plt.bar(range(X_train.shape[1]), importances[sorted_idx], align='center')

# Add labels for each feature
plt.xticks(range(X_train.shape[1]), np.array(feature_names)[sorted_idx], rotation=90)

# Add labels for each FMSE value
for i, v in enumerate(["{:.4f}".format(val) for val in fmse[sorted_idx]]):
  plt.text(i, importances[sorted_idx][i], str(v), color='red', fontweight='regular', rotation=45)

# Set plot title and axis labels
plt.title("Feature Importances")
plt.xlabel("Features")
plt.ylabel("Importance")

# Show the plot
plt.show()